import numpy as npimport pandas as pdfrom sklearn.model_selection import GridSearchCVimport matplotlib#matplotlib.use('Agg')from matplotlib import pyplot as plttrain = pd.read_csv('FE_pima-indians-diabetes.csv')y_train = train['Target']X_train = train.drop(['Target'],axis=1)X_train.head()#训练特征的名字feat_names = X_train.columnsfrom scipy.sparse import csr_matrix #scipy矩阵运算更快，因为使用了稀疏矩阵X_train = csr_matrix(X_train) #将训练集划分成稀疏矩阵#-log损失，交叉验证分数from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import cross_val_scorelr = LogisticRegression()loss = cross_val_score(lr,X_train,y_train,cv=5,scoring='neg_log_loss')print('log loss of each model is: ',-loss)print('cv logloss is',-loss.mean())from sklearn.model_selection import GridSearchCVfrom sklearn.linear_model import LogisticRegression#超参数调教penaltys = ['l1','l2']Cs = [0.1,1,10,100,1000]tuned_parameters = dict(penalty = penaltys,C=Cs)lr_penalty = LogisticRegression(solver='liblinear')grid = GridSearchCV(lr_penalty,tuned_parameters,cv=5,scoring='neg_log_loss',n_jobs=4,return_train_score = True)grid.fit(X_train,y_train)#examine the best modelprint(-grid.best_score_)print(grid.best_params_)#plot CV误差曲线test_means = grid.cv_results_['mean_test_score'] #测试集的平均值test_stds = grid.cv_results_['std_test_score'] #标准差train_means = grid.cv_results_['mean_train_score'] #训练集的平均误差train_stds = grid.cv_results_['std_test_score']#plot resultsn_Cs = len(Cs)number_penaltys = len(penaltys)test_scores = np.array(test_means).reshape(n_Cs,number_penaltys)train_scores = np.array(train_means).reshape(n_Cs,number_penaltys)test_stds = np.array(test_stds).reshape(n_Cs,number_penaltys)train_stds = np.array(train_stds).reshape(n_Cs,number_penaltys)x_axis = np.log10(Cs)for i,value in enumerate(penaltys):    #pyplot.plot(log(Cs)),test_scores[i],label = 'penalty' +str(value)    plt.errorbar(x_axis,-test_scores[:,i],yerr=test_stds[:,i] ,label = penaltys[i] +' Test')    plt.errorbar(x_axis,-train_scores[:,i],yerr=test_stds[:,i] ,label = penaltys[i] +' Test')plt.legend()plt.xlabel('log(C)')plt.ylabel('logloss')p1 = plt.savefig('LogisticGridsearchCV_C.png')#plt.show(p1)'''ssssss '''from sklearn.model_selection import GridSearchCVfrom sklearn.linear_model import LogisticRegressionpenaltys = ['l1','l2']Cs = [ 0.1, 1, 10, 100, 1000]tuned_parameters = dict(penalty = penaltys, C = Cs)lr_penalty= LogisticRegression(solver='liblinear')grid= GridSearchCV(lr_penalty, tuned_parameters,cv=5, scoring='accuracy',n_jobs = 4,return_train_score=True)grid.fit(X_train,y_train)print(grid.best_score_)print(grid.best_params_)test_means = grid.cv_results_['mean_test_score']test_stds = grid.cv_results_['std_test_score']train_means = grid.cv_results_['mean_train_score']train_stds = grid.cv_results_['std_train_score']# plot resultsn_Cs = len(Cs)number_penaltys = len(penaltys)test_scores = np.array(test_means).reshape(n_Cs, number_penaltys)train_scores = np.array(train_means).reshape(n_Cs, number_penaltys)test_stds = np.array(test_stds).reshape(n_Cs, number_penaltys)train_stds = np.array(train_stds).reshape(n_Cs, number_penaltys)x_axis = np.log10(Cs)for i, value in enumerate(penaltys):    # pyplot.plot(log(Cs), test_scores[i], label= 'penalty:'   + str(value))    plt.errorbar(x_axis, test_scores[:, i], yerr=test_stds[:, i], label=penaltys[i] + ' Test')    plt.errorbar(x_axis, train_scores[:, i], yerr=train_stds[:, i], label=penaltys[i] + ' Train')plt.legend()plt.xlabel('log(C)')plt.ylabel('accuracy')p2 = plt.savefig('LogisticGridSearchCV_C.png')plt.show(p2)